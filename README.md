#  WPAM

## Quick start

Note this is an iOS app and in order to run it you will need a machine with MacOS and physical phone with iOS

1. Connect your iOS device to macOS with xCode installed
2. Open `AccessibilityDemoApp.xcodeproj`
3. Select your physical device as a run destination
4. Run project (CMD + R)

## System requirements

* Xcode 16+
* MacOS 15+
* iOS 18+

## About app

This project was developed as part of the WPAM course at the Warsaw University of Technology.

App is a demo containing:
1. Comparison of accessible and inaccessible design showcasing good SwiftUI practices
2. UX design of the accessible app for exploring the universe:
    * [link to design](UX.md)
    * simplyfied core functionalities are implemented in the app

### Core functionalities

#### Guidance to celestial bodies

User can locate chosen celestial body. Except rendering objects the app will play a tone, which will change its pitch accordingly to the distance between where user is aiming device and the object. When user will successfully locate the target, the phone will vibrate.

#### Universe wiki

We have mocked trivia about Venus that can enlarge user knowledge. This functionality has a great potential to provide knowledge to all users despite theirs capabilities. The trivia would be generated by fast LLM model (f. ex. gemini 2.0 flash) as the answer to users query with provided context about celestial body, users location and curated documents with information about space objects.

#### Exploring cosmos

User can explore some celestial bodies with haptic and audio feedback.

#### Haptics demo

List of buttons that invoke different vibration patters. Its purpuse is to showcase what iPhone haptic engine is capable of.

#### Exposed intent to OS

We exposed one of ours core functionalities (Venus Wiki) to iOS. This enables the user to use it outside the app:
* creating shortcut
* accessing it via Spotlight and Siri
* using it automatically in focus modes

### Native libraries

We used:
* `SwiftUI` for native visual elements
* `CoreHaptics` to enhance user experience with more advance haptic feedback than SwiftUI has to offer
* `AVKit` and `AVFoundation` for generating and managing audio
* `AppIntents` to expose core functionalities to the system
* `ARKit` and `RealityKit` for building, rendering and interacting with augmented reality views
* `Combine` for asynchronous events orchestration

### Third party libraries

We used:
* AudioManager by Robin Lieb

## Authors

Jan Stusio

Krzysztof Å»ak

